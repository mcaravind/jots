{"text":"{|17160|#|/n/ /st/The likelihood value/ |}{|60960|#|/n//n/We can separate this into two components|}{|74960|#|/n/The derivative is the difference between the derivatives of each component|}{|88980|#|/n/ /st/The derivative I: Numerator/ |}{|159500|#|/n/Numerator has a simple form|}{|168240|#|/n/ /st/The derivative II: Denominator/ |}{|343500|#|/n/ /st/The derivative III/ |}{|355000|#|/n/Actual count of feature - predicted count of feature|}{|405260|#|/n/The optimum distribution is always unique, and always exists|}{|418020|#|/n/ /st/Finding the optimal parameters/ |}{|433520|#|/n/We want to choose parameters that maximize the conditional log likelihood of the training data|}{|452000|#|/n/ /st/A likelihood surface/ |}{|525420|#|/n/ /st/Finding the optimal parameters/ |}{|533940|#|/n/Use a good numerical optimization package|}{|555440|#|/n/1.Gradient descent; stochastic gradient descent|}{|575440|#|/n/-often used for big problems|}{|581200|#|/n/2.Iterative proportional fitting methods|}{|592940|#|/n/3.Conjugate gradient|}{|600520|#|/n/4.Quasi Newton methods - preferred these days|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}\n","videoid":"InuXtFCr3WA","title":"8 - 6 - Maximizing the Likelihood- Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":629.601}