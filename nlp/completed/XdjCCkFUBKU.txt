{"text":"{|2181|#|Advanced methods of smoothing|}{|8181|#|/n/ /st/Add-1 smoothing/ |}{|17601|#|/n/ /st/Add-k smoothing/ |}{|27101|#|/n/Replace with new variable m = kV|}{|44121|#|/n/ /st/Unigram prior smoothing/ |}{|50921|#|/n/Adding to every bigram a constant|}{|60421|#|/n/Unigram probability of word we are backing off to|}{|73901|#|/n/ $P_{Add-k}(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_1)+m(1/V)}{c(w_{i-1})+m}$ |}{|73902|#|/n/ $P_{UnigramPrior}(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i) + mP(w_i)}{c(w_{i-1})+m}$ |}{|99661|#|/n/Although it works well, not well enough for language modeling|}{|113422|#|/n/ /st/Advanced smoothing algorithms/ |}{|121182|#|/n/Intuition - use count of things we've seen once to help estimate count of things we've never seen|}{|143522|#|/n/ /st/Notation: $N_c$ = Frequency of frequency c/ |}{|167582|#|/n/$N_c$ = the count of things we've seen c times|}{|179302|#|/n/Example|}{|220842|#|/n/ /st/Good-Turing smoothing intuition/ |}{|232362|#|/n/Fishing - how likely is it that the next species is trout?|}{|266902|#|/n/How likely is that the next species is new|}{|280942|#|/n/ - use estimate of things we saw once to estimate new things|}{|329522|#|/n/So how likely is it that next species is trout?|}{|369882|#|/n/ /st/Good Turing calculations/ |}{|385882|#|/n/$P_{GT}^* (things with zero frequency) = \\frac{N_1}{N}    c^* =  \\frac{(c+1)N_{c+1}}{N_c} $|}{|416543|#|/n/Unseen probability|}{|529303|#|/n/ /st/Ney et al.s Good Turing Intuition/ |}{|549043|#|/n/Held out words|}{|600663|#|/n/Dan Klein intuition|}{|627183|#|/n/What fraction of held out words are unseen in training?|}{|679003|#|/n/What fraction of held out words are seen k times in training?|}{|773263|#|/n/ Expected count : $k^* = \\frac{(k+1)N_{k+1}}{N_k}$|}{|790883|#|/n/ /st/Good-Turing complications (from Dan Klein)/ |}{|803903|#|/n/What about \"the\"?|}{|814403|#|/n/For large k, too jumpy, zeros wreck estimates|}{|840963|#|/n/Simple good-turing: replace empirical $N_k$ with a best fit power law once count get unreliable|}{|864924|#|/n/ /st/Resulting Good-Turing numbers/ |}{|877944|#|/n/Numbers from Church and Gale|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"XdjCCkFUBKU","title":"4 - 7 - Good-Turing Smoothing - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":935.441}