{"text":"{|9701|#|/n/ /st/The Shannon Visualization Method/ |}{|23721|#|/n/Choose a random bigram according to its probability|}{|43501|#|/n/Now choose another bigram chosen according to its probability|}{|59541|#|/n/Until we get the entire sentence|}{|76321|#|/n/ /st/Approximating Shakespeare/ |}{|85581|#|/n/Generates random sentences after training on Shakespeare|}{|119101|#|/n/ /st/Shakespeare as corpus/ |}{|130641|#|/n/N = 884647 tokens, V = 29066|}{|166181|#|/n/Quadrigram looks like Shakespear because it actually is Shakespeare|}{|175063|#|/n/ /st/The wall street journal is not shakespeare/ |}{|193063|#|/n/Trigrams are not as convincing|}{|211863|#|/n/ /st/The perils of overfitting/ |}{|219123|#|/n/N-grams only work well if test corpus looks like training corpus|}{|244923|#|/n/ /st/Zeros/ |}{|249943|#|/n/Training set|}{|297043|#|/n/ /st/Zero probability bigrams/ |}{|305803|#|/n/Bigrams with zero probability means that we will assign 0 probability to the test set - we need a better method|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}\n","videoid":"s5Yg6qac9ag","title":"4 - 4 - Generalization and Zeros - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":315.261}