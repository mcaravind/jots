{"text":"{|1400|#|How do we deal with bigrams with zero probability?|}{|8741|#|/n/ /st/Intuition of smoothing/ |}{|35001|#|/n/We have sparse statistics|}{|50001|#|/n/We like to steal some probability mass to generalize better|}{|69281|#|/n/ /st/Add-one estimation or Laplace smoothing/ |}{|79801|#|/n/We pretend we saw a word one more time than we actually did|}{|88241|#|/n/MLE estimate vs Add-1 estimate formula|}{|132301|#|/n/ /st/Maximum likelihood estimates/ |}{|141840|#|/n/Maximizes the likelihood of the training set given the model|}{|185341|#|/n/MLE can be a bad estimate for some other corpus|}{|206141|#|/n/Any smoothing is not MLE|}{|214161|#|/n/ /st/Berkeley Restaurant corpus/ |}{|230181|#|/n/ /st/Laplace-smoothed bigrams/ |}{|241441|#|/n/Add one smoothed bigrams computed|}{|255201|#|/n/ /st/Reconstituted counts/ |}{|285241|#|/n/How much did add one smoothing change our probabilities?|}{|304500|#|/n/Compare with original coutns|}{|317500|#|/n/There is a huge change|}{|358801|#|/n/ /st/Add-1 estimation is a very blunt instrument/ |}{|373321|#|/n/We dont use it in practice|}{|379821|#|/n/But we do use it for other NLP models|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"d8nVJjlMOYo","title":"4 - 5 - Smoothing_ Add-One - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":390.481}