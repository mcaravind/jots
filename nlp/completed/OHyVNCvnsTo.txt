{"text":"{|63601|#|/n/We need an evaluation metric|}{|65841|#|/n/ /st/Extrinsic evaluation of N-gram models/ |}{|80401|#|/n/Put each model in a task and run the task|}{|93961|#|/n/Compary accuracy for models, say A and B|}{|119801|#|/n/ /st/Difficulty of extrinsic (in-vivo) evaluation of N-gram models/ |}{|128402|#|/n/Time consuming, can take days or weeks|}{|139142|#|/n/We sometimes use intrinsic evaluation - most common is perplexity|}{|155662|#|/n/It is a bad approximation, but generally useful in pilot experiments|}{|176182|#|/n/ /st/Intuition of perplexity/ |}{|186702|#|/n/The Shannon game: how well can we predict the next word?|}{|260101|#|/n/Unigrams are terrible at this game|}{|267121|#|/n/A better model of text assigns a higher probability to the word that actually occurs|}{|278961|#|/n/ /st/Perplexity/ |}{|286961|#|/n/Best language model best predicts an unseen test set|}{|306061|#|/n/Perplexity is the probability of the test set normalized by the number of words|}{|309742|#|/n/ $PP(W)=P(w_1w_2...w_N)^{1/N}$ |}{|363722|#|/n/Chain rule|}{|396562|#|/n/Bigrams|}{|424842|#|/n/ /st/The Shannon Game intuition for perplexity/ |}{|438142|#|/n/Related to Average branching factor|}{|522742|#|/n/Perplexity is the weighted equivalent branching factor|}{|526942|#|/n/ /st/Perplexity as branching factor/ |}{|544362|#|/n/Sentence consisting of random digits|}{|554462|#|/n/What is the perplexity of this sentence according to a model that assign P = 1/10 to each digit?|}{|611042|#|/n/Perplexity value is 10 = branching factor|}{|627802|#|/n/ /st/Lower perplexity = better model/ |}{|643122|#|/n/Training 38 million words, test 1.5 million words, WSJ|}{|658962|#|/n/Unigram vs bigram vs trigram|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"OHyVNCvnsTo","title":"4 - 3 - Evaluation and Perplexity - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":669.021}