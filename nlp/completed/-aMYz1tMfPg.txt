{"text":"{|4121|#|Cases where we need to interpolate, or backoff, from one language model to another one|}{|11641|#|/n/ /st/Backoff and implementation/ |}{|18921|#|/n/Use less context|}{|36201|#|/n/Backoff - use trigram if you have good evidence, otherwise bigram, otherwise unigram|}{|52281|#|/n/Interpolation - mix unigram, bigram and trigram|}{|76801|#|/n/In practice, interpolation works better|}{|83561|#|/n/ /st/Linear interpolation/ |}{|93061|#|/n/Simple interpolation|}{|118061|#|/n/Lambdas conditional on context|}{|142841|#|/n/ /st/How to set the lambdas/ |}{|151081|#|/n/Using a held-out corpus|}{|176861|#|/n/Choose lambdas which maximize the likelihood of the held-out data|}{|222941|#|/n/ /st/Unknown words: open versus closed vocabulary tasks/ |}{|238461|#|/n/If we know all the words in advance - closed vocabulary task|}{|264221|#|/n/OOV - out of vocabulary words|}{|273481|#|/n/We create a special token called &lt;UNK&gt;|}{|302883|#|/n/Use low probability words, replace with UNK, and train as if the low prob words were UNKs|}{|328663|#|/n/ /st/Huge web-scale n-grams/ |}{|336923|#|E.g. Google N-Gram corpus|}{|350423|#|/n/Pruning - only store N-grams with count &gt; threshold|}{|367563|#|/n/Entropy based pruning|}{|377603|#|/n/Efficiency|}{|385483|#|/n/ - data structures like tries|}{|393483|#|/n/ - bloom filters|}{|398723|#|/n/ -store words as indexes not string|}{|408743|#|/n/ - quantize probabilities|}{|409944|#|/n/ /st/Smoothing for web scale N-grams/ |}{|418724|#|/n/Stupid backoff - most popular|}{|448004|#|/n/Use Maximum likelihood estimator if &gt; 0|}{|461864|#|/n/else backoff to lower order prefix|}{|521424|#|/n/ /st/N-gram smoothing summary/ |}{|528004|#|/n/Add 1 smoothing ok for text categorization, not for language modeling|}{|534984|#|/n/Most common - extended interpolated kneser-ney|}{|540565|#|/n/for very large n-grams - stupid backoff|}{|551565|#|/n/ /st/Advanced language modeling/ |}{|558185|#|/n/Discriminative models - choose n gram weights to improve a task, not fit the training set|}{|579485|#|/n/Parsing-based models|}{|589785|#|/n/Caching models - recently used word more likely to appear again|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}\n","videoid":"-aMYz1tMfPg","title":"4 - 6 - Interpolation - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":625.452175}