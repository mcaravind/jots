{"text":"{|3161|How do we estimate n gram probabilities?|}{|4824|/n/ /st/Estimating bigram probabilities/ |}{|17604|/n/Counting|}{|47164|/n/ /st/An example/ |}{|164664|/n/ /st/More examples: Berkeley Restaurant Project sentences/ |}{|190204|/n/Compute n grams based on these sentences|}{|192886|/n/ /st/Raw bigram counts/ |}{|201406|/n/From small corpus of about 10000 sentences|}{|279006|/n/ /st/Raw bigram probabilities - 2/ |}{|305026|/n/to given want is quite likely|}{|311667|/n/ /st/Bigram estimates of sentence probabilities/ |}{|322687|/n/Simply multiply all the component probabilities|}{|339888|/n/ /st/What kinds of knowledge expressed by bigram probabilities?/ |}{|355448|/n/facts about world (cuisines)|}{|374208|/n/facts about grammar|}{|439268|/n/ /st/Practical issues/ |}{|446028|/n/We keep them in the form of log probabilities to avoid underflow when multiplying small probabilities|}{|475588|/n/Adding is faster than multiplying too|}{|484588|/n/ /st/Language modeling toolkits/ |}{|494868|/n/SRILM|}{|502868|/n/Google N-Gram release|}{|511948|/n/Huge corpus|}{|522948|/n/ /st/Google N-Gram release/ |}{|548488|/n/ /st/Google Book N-Grams corpus/ |}{|563768|/n/Counts of words in Google Books|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"o-CvoOkVrnY","title":"4 - 2 - Estimating N-gram Probabilities - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":578.06077}