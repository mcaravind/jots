{"text":"{|6400|#|Language modeling is one of the most important topics in natural language processing|}{|7021|#|/n/ /st/Probabilistic Language Models/ |}{|13561|#|/n/Assign a probability to a sentence|}{|20321|#|/n/Machine translation|}{|30581|#|/n/Spell correction|}{|46601|#|/n/Speech recognition|}{|60881|#|/n/Summarization, question answering and many places|}{|65782|#|/n/ /st/Probabilistic Language Modeling - 2/ |}{|77302|#|/n/Goal: compute probability of sentence or sequence of words|}{|83183|#|/n/Related task: probability of an upcoming word|}{|99723|#|/n/A model that computes either of these is called a language model|}{|129631|#|/n/ /st/How to compute P(W)/ |}{|133491|#|/n/Intuition - rely on the chain rule of probability|}{|147014|#|/n/ /st/Reminder: The Chain Rule/ |}{|194335|#|/n/Generalize to more variables|}{|209137|#|/n/general form of chain rule|}{|224136|#|/n/ /st/The chain rule applied to compute joint probability of words in sentence/ |}{|249279|#|/n/Formal definition|}{|260079|#|/n/ /st/How to estimate these probabilities?/ |}{|271459|#|/n/Can we just count and divide?|}{|287979|#|/n/we cannot - too many possible sentences|}{|300059|#|/n/ /st/Markov Assumption/ |}{|309079|#|/n/Estimate by using only previous word(s)|}{|343979|#|/n/Formal definition|}{|370319|#|/n/ /st/Simplest case of Markov model: unigram model/ |}{|385979|#|/n/Automatically generated sentences from a unigram model|}{|406239|#|/n/ /st/Bigram mode/ |}{|411319|#|/n/Slightly more intelligent than unigram model|}{|451239|#|/n/ /st/N-gram models/ |}{|457559|#|/n/Insufficient still - since language has long distance dependencies|}{|511739|#|/n/in practice, we can get away with n-gram models|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"s3kKlUBa3b0","title":"4 - 1 - Introduction to N-grams- Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":521.14857}