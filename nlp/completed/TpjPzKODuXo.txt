{"text":"{|3141|#|/n/ /st/Bayes rule applied to documents and classes/ |}{|15901|#|/n/For a document d and a class c calculating the conditional probability given the document|}{|47321|#|/n/ /st/Naive bayes classifier/ |}{|58101|#|/n/The best class is the one that maximizes the probability of that class given the document|}{|99604|#|/n/ $$C_{MAP} = \\max\\limits_{c \\varepsilon C} P(d | c) P (c) $$|}{|99986|#|/n/We can drop the denominator because the probability of the document is identical for all classes|}{|176546|#|/n/ /st/Naive Bayes classifier/ |}{|188306|#|/n/Probability of document given class?|}{|211326|#|/n/-represent the document with a vector of features|}{|242126|#|/n/ /st/Naive Bayes classifier - IV/ |}{|256546|#|/n/How often does this occur?|}{|271566|#|/n/We can count relative frequencies to get probability of class|}{|317666|#|/n/Simplifying assumptions:|}{|324966|#|/n/ /st/Multinomial Naive Bayes independence assumptions/ |}{|338486|#|/n/Bag of words assumption : assume position doesn't matter|}{|350986|#|/n/Conditional independence - assume feature probabilities are independent given the class|}{|374746|#|/n/These are incorrect, but nonetheless we can make the problem much simpler|}{|436066|#|/n/ /st/Multinomail Naive Bayes classifier/ |}{|488366|#|/n/ /st/Applying multinomial naive bayes classifiers to text classification/ |}{|508146|#|/n/For each position, look at word at position and their probabilities|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"TpjPzKODuXo","title":"6 - 3 - Formalizing the Naive Bayes Classifier - Stanford NLP-Dan Jurafsky & Chris Manning","duration":569.027345}