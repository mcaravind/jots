{"text":"{|8864|#|/n/ /st/Resulting Good-Turing numbers/ |}{|23184|#|/n/Discounted different counts|}{|59764|#|/n/In practice, Good Turing often produces a fixed small discount from the original counts|}{|68164|#|/n/ /st/Absolute discounting interpolation/ |}{|84024|#|/n/$P_{AbsoluteDiscounting}(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i) - d}{c(w_{i-1})} + \\lambda (w_{i-1}) P(w)$|}{|123544|#|/n/maybe keep a couple of extra values of d for counts of 1 and 2|}{|138324|#|/n/But should we really just use the regular unigram|}{|152344|#|/n/ /st/Kneser-Ney smoothing/ |}{|159844|#|/n/Better estimate for probabilities of lower order unigrams|}{|203144|#|/n/Unigram is useful exactly when we haven't seen this bigram|}{|228664|#|/n/Instead of \"How likely is w\", we use \"How likely is w to appear as a novel continuation?\"|}{|235344|#|/n/ - for each word, count number of bigram types it completes|}{|259584|#|/n/ $P_{CONTINUATION} (w) \\propto |\\{w_{i-1} : c(w_{i-1},w)&gt;0 \\}|$|}{|283324|#|/n/ /st/Kneser-Ney smoothing II / |}{|297104|#|/n/Normalized by the total number of word bigram types|}{|312624|#|/n/ /st/Kneser-Ney smoothing III/ |}{|321784|#|/n/Alternative metaphor: the number of # of word types seen to precede w|}{|342084|#|/n/normalized by the # of words preceding all words|}{|368864|#|/n/ /st/Kneser-Ney smoothing IV/ |}{|377843|#|/n/ $P_{KN}(w_i | w_{i-1}) = \\frac{max(c(w_{i-1},w_i)-d,0)}{c(w_{i-1})}+\\lambda(w_{i-1})P_{CONTINUATION}(w_i)$|}{|425424|#|/n/$\\lambda$ is normalizing constant, the probability mass we've discounted|}{|462724|#|/n/ /st/Kneser-Ney smoothing: Recursive formulation/ |}{|477484|#|/n/Continuation count = Number of unique single word contexts for dot|}","css":".st{font-weight:bold;}\n.st:before{\n    content:\"\\000A\";\n    white-space: pre;\n}","videoid":"wtB00EczoCM","title":"4 - 8 - Kneser-Ney Smoothing - Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":539.301}